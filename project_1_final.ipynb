{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport graphviz\nimport subprocess\nfrom sklearn import datasets, tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, make_scorer, plot_roc_curve, roc_curve, auc\nfrom sklearn.preprocessing import LabelEncoder\nfrom os import system\nfrom IPython.display import Image\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-12-16T21:41:31.556402Z","iopub.execute_input":"2021-12-16T21:41:31.557020Z","iopub.status.idle":"2021-12-16T21:41:32.973659Z","shell.execute_reply.started":"2021-12-16T21:41:31.556907Z","shell.execute_reply":"2021-12-16T21:41:32.972655Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Read the csv file from the local machine","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/adult-data/adult-dataset.csv\")\ndf.head()\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:14.386134Z","iopub.execute_input":"2021-12-16T20:56:14.386417Z","iopub.status.idle":"2021-12-16T20:56:14.505419Z","shell.execute_reply.started":"2021-12-16T20:56:14.386382Z","shell.execute_reply":"2021-12-16T20:56:14.504157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load test data\n Pre-process the test set as same as train set.","metadata":{}},{"cell_type":"code","source":"names = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"class-label\"]\ndf_test = pd.read_csv(\"../input/adult-data/adult.test\",names = names)\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:18.471192Z","iopub.execute_input":"2021-12-16T20:56:18.472253Z","iopub.status.idle":"2021-12-16T20:56:18.523173Z","shell.execute_reply.started":"2021-12-16T20:56:18.472201Z","shell.execute_reply":"2021-12-16T20:56:18.521915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing data\nWe find out that many instances contain a question mark (missing data). Then, we decided to remove this data from our analysis","metadata":{}},{"cell_type":"code","source":"df = df.replace({' ?': np.nan})\ndf.dropna(how='any',inplace = True)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:20.569987Z","iopub.execute_input":"2021-12-16T20:56:20.570382Z","iopub.status.idle":"2021-12-16T20:56:20.656277Z","shell.execute_reply.started":"2021-12-16T20:56:20.570352Z","shell.execute_reply":"2021-12-16T20:56:20.655248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Univariate analysis ","metadata":{}},{"cell_type":"code","source":"df['class-label'].value_counts().plot(kind = \"bar\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:22.997909Z","iopub.execute_input":"2021-12-16T20:56:22.998157Z","iopub.status.idle":"2021-12-16T20:56:23.134235Z","shell.execute_reply.started":"2021-12-16T20:56:22.998129Z","shell.execute_reply":"2021-12-16T20:56:23.133692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist(column = \"age\",bins = 20)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:25.311623Z","iopub.execute_input":"2021-12-16T20:56:25.311939Z","iopub.status.idle":"2021-12-16T20:56:25.485000Z","shell.execute_reply.started":"2021-12-16T20:56:25.311911Z","shell.execute_reply":"2021-12-16T20:56:25.484149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['workclass'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:16.497708Z","iopub.status.busy":"2021-11-28T10:21:16.497399Z","iopub.status.idle":"2021-11-28T10:21:16.720498Z","shell.execute_reply":"2021-11-28T10:21:16.719859Z","shell.execute_reply.started":"2021-11-28T10:21:16.497675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['education'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:16.721843Z","iopub.status.busy":"2021-11-28T10:21:16.721535Z","iopub.status.idle":"2021-11-28T10:21:17.017563Z","shell.execute_reply":"2021-11-28T10:21:17.016879Z","shell.execute_reply.started":"2021-11-28T10:21:16.721816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['marital-status'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:17.019024Z","iopub.status.busy":"2021-11-28T10:21:17.018652Z","iopub.status.idle":"2021-11-28T10:21:17.221918Z","shell.execute_reply":"2021-11-28T10:21:17.221073Z","shell.execute_reply.started":"2021-11-28T10:21:17.018992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['occupation'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:17.223545Z","iopub.status.busy":"2021-11-28T10:21:17.223183Z","iopub.status.idle":"2021-11-28T10:21:17.492396Z","shell.execute_reply":"2021-11-28T10:21:17.491388Z","shell.execute_reply.started":"2021-11-28T10:21:17.22346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['relationship'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:17.49448Z","iopub.status.busy":"2021-11-28T10:21:17.494145Z","iopub.status.idle":"2021-11-28T10:21:17.675153Z","shell.execute_reply":"2021-11-28T10:21:17.674504Z","shell.execute_reply.started":"2021-11-28T10:21:17.494425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['race'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:17.677233Z","iopub.status.busy":"2021-11-28T10:21:17.677007Z","iopub.status.idle":"2021-11-28T10:21:17.850798Z","shell.execute_reply":"2021-11-28T10:21:17.849389Z","shell.execute_reply.started":"2021-11-28T10:21:17.677205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sex'].value_counts().plot(kind = \"bar\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:17.852976Z","iopub.status.busy":"2021-11-28T10:21:17.852664Z","iopub.status.idle":"2021-11-28T10:21:18.018078Z","shell.execute_reply":"2021-11-28T10:21:18.017108Z","shell.execute_reply.started":"2021-11-28T10:21:17.852934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the previous bar chart, it shows that most of the data belong to United-states. Therefore, we can remove it from our analysis.","metadata":{}},{"cell_type":"markdown","source":"Because the capital-loss and capital-gain are numeric (most of them distributed in a wide range), we decided to normalize the data. ","metadata":{}},{"cell_type":"code","source":"df[\"capital-gain\"]=((df[\"capital-gain\"]-df[\"capital-gain\"].min())/(df[\"capital-gain\"].max()-df[\"capital-gain\"].min()))\nbins= [0, 0.05, 0.1, 0.15, 0.25, 0.3, 0.35, 0.40, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\nplt.hist(df[\"capital-gain\"], bins=bins, edgecolor=\"k\")\nplt.xlabel('Capital_Gain')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:28.732921Z","iopub.execute_input":"2021-12-16T20:56:28.734119Z","iopub.status.idle":"2021-12-16T20:56:28.872021Z","shell.execute_reply.started":"2021-12-16T20:56:28.734052Z","shell.execute_reply":"2021-12-16T20:56:28.871090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"capital-loss\"]=((df[\"capital-loss\"]-df[\"capital-loss\"].min())/(df[\"capital-loss\"].max()-df[\"capital-loss\"].min()))\nbins= [0, 0.05, 0.1, 0.15, 0.25, 0.3, 0.35, 0.40, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\nplt.hist(df[\"capital-loss\"], bins=bins, edgecolor=\"k\")\nplt.xlabel('Capital_Loss')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:36:20.04599Z","iopub.execute_input":"2021-12-16T20:36:20.046271Z","iopub.status.idle":"2021-12-16T20:36:20.208904Z","shell.execute_reply.started":"2021-12-16T20:36:20.046243Z","shell.execute_reply":"2021-12-16T20:36:20.20815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"fnlwgt\"]=((df[\"fnlwgt\"]-df[\"fnlwgt\"].min())/(df[\"fnlwgt\"].max()-df[\"fnlwgt\"].min()))\n\nbins= [0, 0.05, 0.1, 0.15, 0.25, 0.3, 0.35, 0.40, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\nplt.hist(df[\"fnlwgt\"], bins=bins, edgecolor=\"k\")\nplt.xticks(bins)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['education-num'].value_counts().plot(kind = \"bar\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins= [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nplt.hist(df[\"hours-per-week\"], bins=bins, edgecolor=\"k\")\nplt.xticks(bins)\nplt.show()\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(column=['age'])","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:18.493441Z","iopub.status.busy":"2021-11-28T10:21:18.492742Z","iopub.status.idle":"2021-11-28T10:21:18.733418Z","shell.execute_reply":"2021-11-28T10:21:18.73257Z","shell.execute_reply.started":"2021-11-28T10:21:18.493396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(column=['fnlwgt'])","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:18.735499Z","iopub.status.busy":"2021-11-28T10:21:18.734847Z","iopub.status.idle":"2021-11-28T10:21:18.9693Z","shell.execute_reply":"2021-11-28T10:21:18.968344Z","shell.execute_reply.started":"2021-11-28T10:21:18.735444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.boxplot(column=['hours-per-week'])","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:18.970859Z","iopub.status.busy":"2021-11-28T10:21:18.970621Z","iopub.status.idle":"2021-11-28T10:21:19.218282Z","shell.execute_reply":"2021-11-28T10:21:19.217401Z","shell.execute_reply.started":"2021-11-28T10:21:18.970823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can drop 'native-country' and 'workclass', 'capital-loss', and 'capital-gain' as features, because they are very imbalanced(dominated by one value), and we assume that the occupation feature is more important than workclass.","metadata":{}},{"cell_type":"code","source":"# Drop \"native-country\",\"workclass\", \"capital-loss\", \"capital-gain\" features\ndf = df.drop(labels=[\"native-country\",\"workclass\", \"capital-loss\", \"capital-gain\"],axis=1)\ndf_bin = pd.get_dummies(df[\"class-label\"])\n\n# Convert class-label to bin\ndf[\"class-label\"]=df_bin.iloc[:,1]\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:41.750448Z","iopub.execute_input":"2021-12-16T20:56:41.750700Z","iopub.status.idle":"2021-12-16T20:56:41.780380Z","shell.execute_reply.started":"2021-12-16T20:56:41.750671Z","shell.execute_reply":"2021-12-16T20:56:41.779409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Transformation\ndf.education = df.education.replace([' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th'],'school')\ndf.education = df.education.replace(' HS-grad','high school')\ndf.education = df.education.replace([' Assoc-voc' , ' Assoc-acdm' , ' prof-school' , ' Some-college'],'higher')\ndf.education = df.education.replace(' Bachelors','undergrad')\ndf.education = df.education.replace (' Masters','grad')\ndf.education = df.education.replace(' Doctorate','doc')\ndf['marital-status'] = df['marital-status'].replace ([' Married-civ-spouse',' Married-AF-spouse'],'married')\ndf['marital-status'] = df['marital-status'].replace ([' Never-married'],'not-married')\ndf['marital-status'] = df['marital-status'].replace ([' Divorced',' Separated',' Widowed',' Married-spouse-absent'],'other')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:45.072335Z","iopub.execute_input":"2021-12-16T20:56:45.072613Z","iopub.status.idle":"2021-12-16T20:56:45.158597Z","shell.execute_reply.started":"2021-12-16T20:56:45.072584Z","shell.execute_reply":"2021-12-16T20:56:45.157698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['class-label'],palette = 'coolwarm' , hue = 'marital-status' , data=df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['class-label'],palette = 'coolwarm' , hue = 'education' , data=df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df['class-label'],palette = 'coolwarm' , hue = 'relationship' , data=df)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bivariate analysis","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(12, 9))\n\nhm = sns.heatmap(df.corr(), annot = True)\n\nhm.set(title = \"Correlation matrix of Adult dataset\\n\")\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2021-11-28T10:21:19.281807Z","iopub.status.busy":"2021-11-28T10:21:19.281498Z","iopub.status.idle":"2021-11-28T10:21:19.813347Z","shell.execute_reply":"2021-11-28T10:21:19.812556Z","shell.execute_reply.started":"2021-11-28T10:21:19.281768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Prossessing test set data","metadata":{}},{"cell_type":"code","source":"# Drop the rows which contain question mark.\ndf_test = df_test.replace({' ?': np.nan})\ndf_test.dropna(how='any',inplace = True)\n\n\n# Remove 'native-country' and 'workclass', 'capital-loss', and 'capital-gain' feature.\ndf_test = df_test.drop(labels=[\"native-country\",\"workclass\", \"capital-loss\", \"capital-gain\"],axis=1)\n\n# Normilize fnlwgt\ndf_test[\"fnlwgt\"]=((df_test[\"fnlwgt\"]-df_test[\"fnlwgt\"].min())/(df_test[\"fnlwgt\"].max()-df_test[\"fnlwgt\"].min()))\n\n# Convert \"class-label\" feature to bin\ndf_bin = pd.get_dummies(df_test[\"class-label\"])\ndf_test[\"class-label\"]=df_bin.iloc[:,1]\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:54.309466Z","iopub.execute_input":"2021-12-16T20:56:54.309736Z","iopub.status.idle":"2021-12-16T20:56:54.396558Z","shell.execute_reply.started":"2021-12-16T20:56:54.309689Z","shell.execute_reply":"2021-12-16T20:56:54.395805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Transformation of test set\ndf_test.education = df_test.education.replace([' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th'],'school')\ndf_test.education = df_test.education.replace(' HS-grad','high school')\ndf_test.education = df_test.education.replace([' Assoc-voc' , ' Assoc-acdm' , ' prof-school' , ' Some-college'],'higher')\ndf_test.education = df_test.education.replace(' Bachelors','undergrad')\ndf_test.education = df_test.education.replace (' Masters','grad')\ndf_test.education = df_test.education.replace(' Doctorate','doc')\ndf_test['marital-status'] = df_test['marital-status'].replace ([' Married-civ-spouse',' Married-AF-spouse'],'married')\ndf_test['marital-status'] = df_test['marital-status'].replace ([' Never-married'],'not-married')\ndf_test['marital-status'] = df_test['marital-status'].replace ([' Divorced',' Separated',' Widowed',' Married-spouse-absent'],'other')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:56:58.030097Z","iopub.execute_input":"2021-12-16T20:56:58.030316Z","iopub.status.idle":"2021-12-16T20:56:58.095043Z","shell.execute_reply.started":"2021-12-16T20:56:58.030292Z","shell.execute_reply":"2021-12-16T20:56:58.094165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label encoding for train set\ndf['education'] = df.apply(LabelEncoder().fit_transform)['education']\ndf['marital-status'] = df.apply(LabelEncoder().fit_transform)['marital-status']\ndf['occupation'] = df.apply(LabelEncoder().fit_transform)['occupation']\ndf['relationship'] = df.apply(LabelEncoder().fit_transform)['relationship']\ndf['race'] = df.apply(LabelEncoder().fit_transform)['race']\ndf[ 'sex'] = df.apply(LabelEncoder().fit_transform)['sex']\n\n# label encoding for test set\ndf_test['education'] = df_test.apply(LabelEncoder().fit_transform)['education']\ndf_test['marital-status'] = df_test.apply(LabelEncoder().fit_transform)['marital-status']\ndf_test['occupation'] = df_test.apply(LabelEncoder().fit_transform)['occupation']\ndf_test['relationship'] = df_test.apply(LabelEncoder().fit_transform)['relationship']\ndf_test['race'] = df_test.apply(LabelEncoder().fit_transform)['race']\ndf_test[ 'sex'] = df_test.apply(LabelEncoder().fit_transform)['sex']\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:00.570276Z","iopub.execute_input":"2021-12-16T20:57:00.570570Z","iopub.status.idle":"2021-12-16T20:57:00.854590Z","shell.execute_reply.started":"2021-12-16T20:57:00.570545Z","shell.execute_reply":"2021-12-16T20:57:00.853815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify Train data\nx_train = df.iloc[:,0:9]\ny_train = df.iloc[:,10]\n\n# Specify Test data\nx_test = df_test.iloc[:,0:9]\ny_test = df_test.iloc[:,10]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:04.889484Z","iopub.execute_input":"2021-12-16T20:57:04.889738Z","iopub.status.idle":"2021-12-16T20:57:04.898354Z","shell.execute_reply.started":"2021-12-16T20:57:04.889686Z","shell.execute_reply":"2021-12-16T20:57:04.897231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Classification","metadata":{}},{"cell_type":"markdown","source":"# 3.3.1 Decision Trees Classification","metadata":{}},{"cell_type":"code","source":"#Train a DT classifier with gini index\n\nclf_gini = tree.DecisionTreeClassifier(random_state=1, criterion=\"gini\")\nclf_gini = clf_gini.fit(x_train,y_train)\ny_pred = clf_gini.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.1.1 Model parameter tuning \n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# defining parameter range\nserach_space ={'criterion': ['gini','entropy'],\n                'max_depth': [7, 8, 9], \n                'min_samples_split': [4, 5], \n                'min_samples_leaf': [4, 5], \n                'max_leaf_nodes':[20, 25, 30]}\ngrid = GridSearchCV(estimator=tree.DecisionTreeClassifier(), param_grid=serach_space, scoring=['accuracy', 'precision', 'recall','f1_macro'], \nrefit='accuracy', cv=10, verbose=5, \nerror_score='raise')\n\n# fitting the model for grid search\ngrid.fit(x_train, y_train)\nbest_dts_model = grid.best_estimator_\nprint ('The best parameters for evaluation are as per following:')\nprint (grid.best_params_)\nprint(\"the accuracy of the best model is  %0.2f\" % grid.best_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.1.2 Evaluation setup & measures\n","metadata":{}},{"cell_type":"code","source":"\n# Measuring Evaluation using accuracy\nscores = cross_val_score(best_dts_model, x_train, y_train, cv=10, scoring='accuracy')\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using f1_macro\nscores = cross_val_score(best_dts_model, x_train, y_train, cv=10, scoring='f1_macro')\nprint(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using sensitivity\nsensitivity = make_scorer(recall_score, pos_label=0)\nscores=cross_val_score(best_dts_model, x_train, y_train, cv=10, scoring=sensitivity)\nprint(\"%0.2f sensitivity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using specificity\nspecificity = make_scorer(recall_score, pos_label=1)\nscores=cross_val_score(best_dts_model, x_train, y_train, cv=10, scoring=specificity)\nprint(\"%0.2f specificity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# confusion matrix \ny_eval = cross_val_predict(best_dts_model, x_train, y_train, cv=10)\nprint ('The Confusion Matrix is:')\nprint (confusion_matrix(y_train, y_eval))\n\n# plot AUC\nfpr, tpr, threshold = roc_curve(y_eval, y_train)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To summarize, we tuned the Decision Tree Classifier using input parameters. We found that \"max_depth\" and \"max_leaf_nodes\" are the most effective parameters which increased the accuracy of our model. It is worth mentioning that using \"gini\" or \"entropy\" criteria didn't change the accuracy of the mentioned model.","metadata":{}},{"cell_type":"code","source":"# predict x_test from Evaluated model in the previous section and calculate precision, recall, f1-score and accuracy\n# Construct the model based on the tuned parameters\nfrom sklearn import datasets, tree\nbest_dts_model = tree.DecisionTreeClassifier(criterion=\"gini\", max_depth = 9, min_samples_split=4, min_samples_leaf=4, max_leaf_nodes=30)\nbest_dts_model = best_dts_model.fit(x_train,y_train)\ny_pred = best_dts_model.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By comparing the measuring criteria before and after evaluation, it is obvious that precision, recall, f1-score, and accuracy have been improved by","metadata":{}},{"cell_type":"markdown","source":"# 3.3.1.3 Model interpretation/visualization\n","metadata":{}},{"cell_type":"markdown","source":"To visualize our model, we used dimention redction.\nThis process is really costly and we just used the 2000 instances of our test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import MDS\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nimport seaborn as sns         \nimport numpy as np\nfrom sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Reduce the dimention of x_test\nx_test_num = x_train.to_numpy()\ndist_manhattan = manhattan_distances(x_test_num[0:2000])\nmds = MDS(dissimilarity='precomputed', random_state=0)\n# Get the embeddings\nx_test_num_L1 = mds.fit_transform(dist_manhattan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model visualization based on best DTs and Dimensionality reduction","metadata":{}},{"cell_type":"code","source":"# Visualize the DTs\ny_pred_num = np.expand_dims(y_pred[0:2000], axis=1)\ny_pred_num = np.append(x_test_num_L1, y_pred_num, axis=1)\ndf_vis_dts = pd. DataFrame(y_pred_num, columns=['dim_1', 'dim_2', 'class-label'])\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data=df_vis_dts , x=\"dim_1\", y=\"dim_2\", hue=\"class-label\", size=\"class-label\",\n    sizes=(100, 20), palette=['dodgerblue','red'], legend=\"full\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz/bin'\ntree.export_graphviz(best_dts_model, out_file='tree.dot')\nsystem(\"dot -Tpng tree.dot -o tree1.png\")\nImage(\"tree1.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.1.4 Discriminative behaviour","metadata":{}},{"cell_type":"code","source":"all_features= pd.concat([x_train,x_test])\ny_pred = clf_gini.predict(all_features)\nfinal_df = all_features.assign(target=y_pred)\n\nplt.figure(figsize=(12, 9))\nhm = sns.heatmap(final_df.corr(), annot = True, cmap=\"crest\")\nhm.set(title = \"Correlation matrix Trained model\\n\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the above Correlation matrix of the model, we can see that there is a positive correlation between \"sex\" and \"target\" which means this model can not avoid discrimination against individuals in the \"sex\" feature. However, this model can avoid discrimination against \"race\" because the correlation is negligible.","metadata":{}},{"cell_type":"markdown","source":"# 3.3.2 KNNs Classification","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Build KNNs model\nfrom sklearn import neighbors\nclf_knn = neighbors.KNeighborsClassifier()\nclf_knn.fit(x_train, y_train)\ny_pred = clf_knn.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:12.811442Z","iopub.execute_input":"2021-12-16T20:57:12.811659Z","iopub.status.idle":"2021-12-16T20:57:13.297982Z","shell.execute_reply.started":"2021-12-16T20:57:12.811636Z","shell.execute_reply":"2021-12-16T20:57:13.297278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.2.1 Model parameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import neighbors\n# defining parameter range\nskf = StratifiedKFold(n_splits=10)\n\nserach_space ={'n_neighbors':[3,5,10,15,20,50],\n              'weights':['uniform','distance'],\n              'metric':['euclidean','manhattan']}\ngrid = GridSearchCV(estimator=neighbors.KNeighborsClassifier(), param_grid=serach_space, scoring=['accuracy', 'precision', 'recall','f1_macro'], \nrefit='accuracy', cv=skf, verbose=1, n_jobs = -1,\nerror_score='raise')\n\n# fitting the model for grid search\ngrid.fit(x_train, y_train)\nbest_knn_model = grid.best_estimator_\nprint ('The best parameters for evaluation are as per following:')\nprint (grid.best_params_)\nprint(\"the accuracy of the best model is  %0.2f\" % grid.best_score_)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:16.058475Z","iopub.execute_input":"2021-12-16T20:57:16.058703Z","iopub.status.idle":"2021-12-16T20:57:30.765685Z","shell.execute_reply.started":"2021-12-16T20:57:16.058680Z","shell.execute_reply":"2021-12-16T20:57:30.764746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.2.2 Evaluation setup ","metadata":{}},{"cell_type":"code","source":"\n# Measuring Evaluation using accuracy\nscores = cross_val_score(best_knn_model, x_train, y_train, cv=10, scoring='accuracy')\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using f1_macro\nscores = cross_val_score(best_knn_model, x_train, y_train, cv=10, scoring='f1_macro')\nprint(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using sensitivity\nsensitivity = make_scorer(recall_score, pos_label=0)\nscores=cross_val_score(best_knn_model, x_train, y_train, cv=10, scoring=sensitivity)\nprint(\"%0.2f sensitivity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using specificity\nspecificity = make_scorer(recall_score, pos_label=1)\nscores=cross_val_score(best_knn_model, x_train, y_train, cv=10, scoring=specificity)\nprint(\"%0.2f specificity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# confusion matrix \ny_eval = cross_val_predict(best_knn_model, x_train, y_train, cv=10)\nprint ('The Confusion Matrix is:')\nprint (confusion_matrix(y_train, y_eval))\n\n# plot AUC\nfpr, tpr, threshold = roc_curve(y_eval, y_train)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:41.918922Z","iopub.execute_input":"2021-12-16T20:57:41.919542Z","iopub.status.idle":"2021-12-16T20:57:48.390739Z","shell.execute_reply.started":"2021-12-16T20:57:41.919516Z","shell.execute_reply":"2021-12-16T20:57:48.390228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The parameters which affect our model are: the neighbors (k), how the label is predicted in relation to the labels of the k nearest neighbours, and what the distance metric is. For example, acording to our parameter tuning above, the best metric for choosing the label is not the majority, but the weighted majority, considering the distances.","metadata":{}},{"cell_type":"code","source":"# Construct the model based on the tuned parameters\nfrom sklearn import neighbors\nbest_knn_model = neighbors.KNeighborsClassifier(metric = 'manhattan', n_neighbors = 50, weights = 'uniform')\nbest_knn_model = best_knn_model.fit(x_train,y_train)\ny_pred = best_knn_model.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:48.392088Z","iopub.execute_input":"2021-12-16T20:57:48.392449Z","iopub.status.idle":"2021-12-16T20:57:48.918658Z","shell.execute_reply.started":"2021-12-16T20:57:48.392417Z","shell.execute_reply":"2021-12-16T20:57:48.917992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.2.3 Model interpretation/visualization","metadata":{}},{"cell_type":"markdown","source":"To visualize our model, we used dimention redction.\nThis process is really costly and we just used the 2000 instances of our test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import MDS\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nimport seaborn as sns         \nimport numpy as np\nfrom sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Reduce the dimention of x_test\nx_test_num = x_train.to_numpy()\ndist_manhattan = manhattan_distances(x_test_num[0:2000])\nmds = MDS(dissimilarity='precomputed', random_state=0)\n# Get the embeddings\nx_test_num_L1 = mds.fit_transform(dist_manhattan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model visualization based on best KNN and Dimensionality reduction","metadata":{}},{"cell_type":"code","source":"# Visualize the KNN\ny_pred_num = np.expand_dims(y_pred[0:2000], axis=1)\ny_pred_num = np.append(x_test_num_L1, y_pred_num, axis=1)\ndf_vis_dts = pd. DataFrame(y_pred_num, columns=['dim_1', 'dim_2', 'class-label'])\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data=df_vis_dts , x=\"dim_1\", y=\"dim_2\", hue=\"class-label\", size=\"class-label\",\n    sizes=(100, 20), palette=['green','orange'], legend=\"full\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.2.4 Discriminative behaviour","metadata":{}},{"cell_type":"code","source":"all_features= pd.concat([x_train,x_test])\ny_pred = best_knn_model.predict(all_features)\nfinal_df = all_features.assign(target=y_pred)\n\nplt.figure(figsize=(12, 9))\nhm = sns.heatmap(final_df.corr(), annot = True, cmap=\"crest\")\nhm.set(title = \"Correlation matrix Trained model\\n\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.3 NBs Classification","metadata":{}},{"cell_type":"code","source":"# Build NBs model\nfrom sklearn.naive_bayes import GaussianNB\nclf_NB = GaussianNB()\nclf_NB.fit(x_train, y_train)\ny_pred = clf_NB.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:54.830895Z","iopub.execute_input":"2021-12-16T20:57:54.831160Z","iopub.status.idle":"2021-12-16T20:57:54.882234Z","shell.execute_reply.started":"2021-12-16T20:57:54.831131Z","shell.execute_reply":"2021-12-16T20:57:54.881511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.3.1 Model parameter tuning","metadata":{}},{"cell_type":"code","source":"# defining parameter range\nskf = StratifiedKFold(n_splits=10)\n\nserach_space ={'var_smoothing': np.logspace(0, -9, num = 100)}\ngrid = GridSearchCV(estimator=GaussianNB(), param_grid=serach_space, scoring=['accuracy', 'recall','f1_macro'], \nrefit='accuracy', cv=skf, verbose=1,n_jobs = -1, \nerror_score='raise')\n\n# fitting the model for grid search\ngrid.fit(x_train, y_train)\nbest_NB_model = grid.best_estimator_\nprint ('The best parameters for evaluation are as per following:')\nprint (grid.best_params_)\nprint(\"the accuracy of the best model is  %0.2f\" % grid.best_score_)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:57:56.670603Z","iopub.execute_input":"2021-12-16T20:57:56.670845Z","iopub.status.idle":"2021-12-16T20:58:02.129644Z","shell.execute_reply.started":"2021-12-16T20:57:56.670814Z","shell.execute_reply":"2021-12-16T20:58:02.129223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.3.2 Evaluation setup ","metadata":{}},{"cell_type":"code","source":"# Measuring Evaluation using accuracy\nscores = cross_val_score(best_NB_model, x_train, y_train, cv=10, scoring='accuracy')\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using f1_macro\nscores = cross_val_score(best_NB_model, x_train, y_train, cv=10, scoring='f1_macro')\nprint(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using sensitivity\nsensitivity = make_scorer(recall_score, pos_label=0)\nscores=cross_val_score(best_NB_model, x_train, y_train, cv=10, scoring=sensitivity)\nprint(\"%0.2f sensitivity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using specificity\nspecificity = make_scorer(recall_score, pos_label=1)\nscores=cross_val_score(best_NB_model, x_train, y_train, cv=10, scoring=specificity)\nprint(\"%0.2f specificity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# confusion matrix \ny_eval = cross_val_predict(best_NB_model, x_train, y_train, cv=10)\nprint ('The Confusion Matrix is:')\nprint (confusion_matrix(y_train, y_eval))\n\n# plot AUC\nfpr, tpr, threshold = roc_curve(y_eval, y_train)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:58:02.130631Z","iopub.execute_input":"2021-12-16T20:58:02.130890Z","iopub.status.idle":"2021-12-16T20:58:02.819080Z","shell.execute_reply.started":"2021-12-16T20:58:02.130867Z","shell.execute_reply":"2021-12-16T20:58:02.818534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict x_test from Evaluate model in the previous section and calculate precision, recall, f1-score and accuracy\nfrom sklearn.naive_bayes import GaussianNB\nbest_NB_model = GaussianNB(var_smoothing = 1e-09)\nbest_NB_model = best_NB_model.fit(x_train,y_train)\ny_pred = best_NB_model.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T20:58:44.569510Z","iopub.execute_input":"2021-12-16T20:58:44.569793Z","iopub.status.idle":"2021-12-16T20:58:44.606672Z","shell.execute_reply.started":"2021-12-16T20:58:44.569763Z","shell.execute_reply":"2021-12-16T20:58:44.606089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.3.3 Model interpretation/visualization","metadata":{}},{"cell_type":"markdown","source":"To visualize our model, we used dimention redction.\nThis process is really costly and we just used the 2000 instances of our test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import MDS\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nimport seaborn as sns         \nimport numpy as np\nfrom sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Reduce the dimention of x_test\nx_test_num = x_train.to_numpy()\ndist_manhattan = manhattan_distances(x_test_num[0:2000])\nmds = MDS(dissimilarity='precomputed', random_state=0)\n# Get the embeddings\nx_test_num_L1 = mds.fit_transform(dist_manhattan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model visualization based on best NB and Dimensionality reduction","metadata":{}},{"cell_type":"code","source":"# Visualize the NB\ny_pred_num = np.expand_dims(y_pred[0:2000], axis=1)\ny_pred_num = np.append(x_test_num_L1, y_pred_num, axis=1)\ndf_vis_dts = pd. DataFrame(y_pred_num, columns=['dim_1', 'dim_2', 'class-label'])\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data=df_vis_dts , x=\"dim_1\", y=\"dim_2\", hue=\"class-label\", size=\"class-label\",\n    sizes=(100, 20), palette=['red','blue'], legend=\"full\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.3.4 Discriminative behaviour","metadata":{}},{"cell_type":"code","source":"all_features= pd.concat([x_train,x_test])\ny_pred = best_NB_model.predict(all_features)\nfinal_df = all_features.assign(target=y_pred)\n\nplt.figure(figsize=(12, 9))\nhm = sns.heatmap(final_df.corr(), annot = True, cmap=\"crest\")\nhm.set(title = \"Correlation matrix Trained model\\n\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.4 Classification SVMs","metadata":{}},{"cell_type":"code","source":"#Train a SVM classifier\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nclf_svm = svclassifier.fit(x_train, y_train)\ny_pred = clf_svm.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.4.1 Model parameter tuning","metadata":{}},{"cell_type":"code","source":"\n# Parameters Tunning and Evaluation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport pickle\n\n\n# defining parameter range\nsvm_linear = {'C': [0.2, 0.3], \n              'kernel': ['linear']} \nsvm_others = {'C': [0.3, 0.5],\n              'gamma': [0.001, 0.003], \n              'kernel': ['rbf', 'sigmoid']}\n\nsvm_poly =  {'C': [0.5, 1], 'gamma': [0.001, 0.003], 'kernel': ['poly'], 'degree': [2, 3]}  # 1, 0.001, 5 acc 0.81\n\nserach_space = [svm_poly, svm_linear, svm_others]  \n \ngrid = GridSearchCV(estimator=SVC(), param_grid=serach_space, scoring=['accuracy', 'precision', 'recall','f1_macro'], \nrefit='accuracy', cv=10, verbose=5, \nerror_score='raise')\n\n# fitting the model for grid search\ngrid.fit(x_train, y_train)\nbest_svm_model = grid.best_estimator_\nprint ('The best parameters for evaluation are as per following:')\nprint (grid.best_params_)\nprint(\"the accuracy of the best model is  %0.2f\" % grid.best_score_)\n\nfilename_svm = 'best_svm_model.sav'\npickle.dump(best_svm_model, open(filename_svm, 'wb'))\n\n# confusion matrix \ny_eval_svm = cross_val_predict(best_svm_model, x_train, y_train, cv=10)\n\nprint ('The Confusion Matrix is:')\nprint (confusion_matrix(y_train, y_eval_svm))\n\n# plot AUC\nfpr, tpr, threshold = roc_curve(y_eval_svm, y_train)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 3.3.4.2 Evaluation setup ","metadata":{}},{"cell_type":"code","source":"# Measuring Evaluation using accuracy\nscores = cross_val_score(best_svm_model, x_train, y_train, cv=10, scoring='accuracy')\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using f1_macro\nscores = cross_val_score(best_svm_model, x_train, y_train, cv=10, scoring='f1_macro')\nprint(\"%0.2f f1_macro with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using sensitivity\nsensitivity = make_scorer(recall_score, pos_label=0)\nscores=cross_val_score(best_svm_model, x_train, y_train, cv=10, scoring=sensitivity)\nprint(\"%0.2f sensitivity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# Measuring Evaluation using specificity\nspecificity = make_scorer(recall_score, pos_label=1)\nscores=cross_val_score(best_svm_model, x_train, y_train, cv=10, scoring=specificity)\nprint(\"%0.2f specificity with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n\n# confusion matrix \ny_eval = cross_val_predict(best_svm_model, x_train, y_train, cv=10)\nprint ('The Confusion Matrix is:')\nprint (confusion_matrix(y_train, y_eval))\n\n# plot AUC\nfpr, tpr, threshold = roc_curve(y_eval, y_train)\nroc_auc = auc(fpr, tpr)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict x_test from Evaluate model in the previous section and calculate precision, recall, f1-score and accuracy\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='poly' , C= 1, degree= 3, gamma= 0.003)\nbest_svm_model = svclassifier.fit(x_train, y_train)\n\ny_pred = best_svm_model.predict(x_test)\nprint(classification_report(y_true = y_test, y_pred = y_pred))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.4.3 Model interpretation/visualization","metadata":{}},{"cell_type":"markdown","source":"To visualize our model, we used dimention redction.\nThis process is really costly and we just used the 2000 instances of our test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import MDS\nfrom matplotlib import pyplot as plt\nimport sklearn.datasets as dt\nimport seaborn as sns         \nimport numpy as np\nfrom sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\n# Reduce the dimention of x_test\nx_test_num = x_train.to_numpy()\ndist_manhattan = manhattan_distances(x_test_num[0:2000])\nmds = MDS(dissimilarity='precomputed', random_state=0)\n# Get the embeddings\nx_test_num_L1 = mds.fit_transform(dist_manhattan)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model visualization based on best SVM and Dimensionality reduction","metadata":{}},{"cell_type":"code","source":"# Visualize the SVM\ny_pred_num = np.expand_dims(y_pred[0:2000], axis=1)\ny_pred_num = np.append(x_test_num_L1, y_pred_num, axis=1)\ndf_vis_dts = pd. DataFrame(y_pred_num, columns=['dim_1', 'dim_2', 'class-label'])\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nsns.scatterplot(data=df_vis_dts , x=\"dim_1\", y=\"dim_2\", hue=\"class-label\", size=\"class-label\",\n    sizes=(100, 20), palette=['black','red'], legend=\"full\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.4.4 Discriminative behaviour","metadata":{}},{"cell_type":"code","source":"all_features= pd.concat([x_train,x_test])\ny_pred = best_svm_model.predict(all_features)\nfinal_df = all_features.assign(target=y_pred)\n\nplt.figure(figsize=(12, 9))\nhm = sns.heatmap(final_df.corr(), annot = True, cmap=\"crest\")\nhm.set(title = \"Correlation matrix Trained model\\n\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3.5 Perceptron","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing, neighbors\nfrom os import system\n\ndf1=df.copy()\ndf1=df1.apply(preprocessing.LabelEncoder().fit_transform)\nss = preprocessing.StandardScaler().fit(df1.drop('class-label',axis=1))\nx_train=ss.transform(df1.drop('class-label',axis=1))\ny_train = df['class-label'].to_numpy()\n\ndf2=df_test.copy()\ndf2=df2.apply(preprocessing.LabelEncoder().fit_transform)\nss = preprocessing.StandardScaler().fit(df2.drop('class-label',axis=1))\nx_test=ss.transform(df2.drop('class-label',axis=1))\ny_test = df['class-label'].to_numpy()\n\nx_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:05:21.310355Z","iopub.execute_input":"2021-12-16T21:05:21.310560Z","iopub.status.idle":"2021-12-16T21:05:21.362713Z","shell.execute_reply.started":"2021-12-16T21:05:21.310537Z","shell.execute_reply":"2021-12-16T21:05:21.361932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\n\n\nclass AdultDataset(object):\n    def __init__(self, x, y):\n        self.x = torch.from_numpy(x)\n        self.y = torch.from_numpy(y)\n        self.n = x.shape[0]\n\n    def __len__(self):\n        return self.n\n\n    def __getitem__(self, idx):\n        return  self.x[idx], self.y[idx]\n\ny_train = y_train.astype(np.int64)\ny_test = y_test.astype(np.int64)\ntrain_data = AdultDataset(x_train, y_train)\ntest_data = AdultDataset(x_test,y_test)\ntrainset = DataLoader(train_data, batch_size = 10, shuffle = True)\ntestset = DataLoader(test_data, batch_size = 10, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:19.450340Z","iopub.execute_input":"2021-12-16T21:07:19.451163Z","iopub.status.idle":"2021-12-16T21:07:19.463429Z","shell.execute_reply.started":"2021-12-16T21:07:19.451122Z","shell.execute_reply":"2021-12-16T21:07:19.461900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perceptron\nclass AdultNet(nn.Module):\n    def __init__(self, input_size , hidden_size, num_classes):\n        super(AdultNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        out = F.relu(self.fc1(x))\n        out = self.fc2(out)\n        return F.log_softmax(out, dim = 1)\nmodel = AdultNet(10,10,2)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:22.171930Z","iopub.execute_input":"2021-12-16T21:07:22.172339Z","iopub.status.idle":"2021-12-16T21:07:22.182396Z","shell.execute_reply.started":"2021-12-16T21:07:22.172301Z","shell.execute_reply":"2021-12-16T21:07:22.181321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = torch.rand(10)\nX = X.view(-1,10)\noutput = model(X)\noutput","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:24.090759Z","iopub.execute_input":"2021-12-16T21:07:24.090980Z","iopub.status.idle":"2021-12-16T21:07:24.098412Z","shell.execute_reply.started":"2021-12-16T21:07:24.090956Z","shell.execute_reply":"2021-12-16T21:07:24.097359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr = 0.001)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:25.704790Z","iopub.execute_input":"2021-12-16T21:07:25.705490Z","iopub.status.idle":"2021-12-16T21:07:25.709339Z","shell.execute_reply.started":"2021-12-16T21:07:25.705446Z","shell.execute_reply":"2021-12-16T21:07:25.708593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 3\nfor epoch in range(EPOCHS):\n    for data in trainset:\n        x, y = data\n        model.zero_grad()\n        output = model(x.view(-1,10).float())\n        loss = F.nll_loss(output, y)\n        loss.backward()\n        optimizer.step()\n    print(loss)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:27.212054Z","iopub.execute_input":"2021-12-16T21:07:27.212266Z","iopub.status.idle":"2021-12-16T21:07:31.711058Z","shell.execute_reply.started":"2021-12-16T21:07:27.212243Z","shell.execute_reply":"2021-12-16T21:07:31.709857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\ntotal = 0\ntrue_pos = 0\ntrue_neg = 0\nfalse_pos = 0\nfalse_neg = 0\nwith torch.no_grad():\n    for data in testset:\n        X, y = data\n        output = model(X.view(-1,10).float())\n        for idx, i in enumerate(output):\n            if torch.argmax(i) == y[idx]:\n                correct +=1\n                if torch.argmax(i):\n                    true_pos += 1\n                else:\n                    true_neg += 1\n            else:\n                if torch.argmax(i):\n                    false_pos += 1\n                else:\n                    false_neg += 1\n            total += 1\nprint(\"Accuracy: \", round(correct/total, 3))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:36.530887Z","iopub.execute_input":"2021-12-16T21:07:36.531153Z","iopub.status.idle":"2021-12-16T21:07:36.963925Z","shell.execute_reply.started":"2021-12-16T21:07:36.531124Z","shell.execute_reply":"2021-12-16T21:07:36.962562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#finding best learning rate.\nimport torch.optim.lr_scheduler as lr_scheduler\n\nlambda_fct = lambda epoch: 0.95**epoch\nscheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda_fct)\nprint(optimizer.state_dict()['param_groups'][0]['lr'])\nfor epoch in range(20):\n    #train\n    model.train()\n    for data in trainset:\n        x, y = data\n        model.zero_grad()\n        optimizer.zero_grad()\n        output = model(x.view(-1,10).float())\n        loss = F.nll_loss(output, y)\n        loss.backward()\n        optimizer.step()\n    #validate:\n    correct_ = 0\n    total_ = 0\n    with torch.no_grad():\n        for data in testset:\n            X, y = data\n            output = model(X.view(-1,10).float())\n            for idx, i in enumerate(output):\n                if torch.argmax(i) == y[idx]:\n                    correct_ +=1\n                total_ += 1\n    print(\"Accuracy: \", round(correct_/total_, 3))\n    #scheduler.step\n    scheduler.step()\n    print(optimizer.state_dict()['param_groups'][0]['lr'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:48.309641Z","iopub.execute_input":"2021-12-16T21:07:48.309973Z","iopub.status.idle":"2021-12-16T21:08:35.229495Z","shell.execute_reply.started":"2021-12-16T21:07:48.309949Z","shell.execute_reply":"2021-12-16T21:08:35.228481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix\ncon_mat = pd.DataFrame({'positive-pred':[true_pos, false_pos],'negative-pred':[false_neg,true_neg]})\ncon_mat","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:08:56.209681Z","iopub.execute_input":"2021-12-16T21:08:56.209981Z","iopub.status.idle":"2021-12-16T21:08:56.221243Z","shell.execute_reply.started":"2021-12-16T21:08:56.209950Z","shell.execute_reply":"2021-12-16T21:08:56.220380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"specificity_1 = true_neg/(true_neg+false_pos)\nsensitivity_1 = true_pos/(true_pos+false_neg)\nprecision_1 = true_pos/(true_pos+false_pos)\nf1_score_1 = ((precision_1*sensitivity_1*2)/(precision_1+sensitivity_1))\nspecificity_0 = true_pos/(true_pos+false_neg)\nsensitivity_0 = true_neg/(true_neg+false_pos)\nprecision_0 = true_neg/(true_neg+false_neg)\nf1_score_0 = ((precision_0*sensitivity_0*2)/(precision_0+sensitivity_0))\nprint(\"specificity for class 0: \", specificity_0)\nprint('sensitivity for class 0: ', sensitivity_0)\nprint('precision for class 0: ', precision_0)\nprint('f1_score for class 0: ',f1_score_0)\nprint(\"specificity for class 1: \", specificity_1)\nprint('sensitivity for class 1: ', sensitivity_1)\nprint('precision for class 1: ', precision_1)\nprint('f1_score for class 1: ',f1_score_1)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:11:55.298656Z","iopub.execute_input":"2021-12-16T21:11:55.299058Z","iopub.status.idle":"2021-12-16T21:11:55.311289Z","shell.execute_reply.started":"2021-12-16T21:11:55.299027Z","shell.execute_reply":"2021-12-16T21:11:55.310281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nacc_result = np.array([[0.83],[0.82], [0.82], [0.82],[0.66]])\nacc_result.reshape(1,5)\ndata = {'Classifier': ['DTs', 'KNN', 'NB', 'svm', 'Perceptron'], 'Accuracy': [0.83, 0.75, 0.76, 0.82, 0.66]}\ndf_result = pd.DataFrame(data)\ndf_result","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:12.618589Z","iopub.execute_input":"2021-12-16T21:27:12.619755Z","iopub.status.idle":"2021-12-16T21:27:12.634674Z","shell.execute_reply.started":"2021-12-16T21:27:12.619679Z","shell.execute_reply":"2021-12-16T21:27:12.633788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns         \nplot_ac = sns.barplot(data =df_result, x = 'Classifier', y='Accuracy', color='blue')\nsns.set(rc={'figure.figsize':(7,8.27)})\n\nplot_ac.set_xticklabels(plot_ac.get_xmajorticklabels(), fontsize = 18)\nplot_ac.set_xlabel(\"Classifier\",fontsize=18)\nplot_ac.set_ylabel(\"Accuracy\",fontsize=18)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:17.290369Z","iopub.execute_input":"2021-12-16T21:27:17.290752Z","iopub.status.idle":"2021-12-16T21:27:17.466852Z","shell.execute_reply.started":"2021-12-16T21:27:17.290707Z","shell.execute_reply":"2021-12-16T21:27:17.466257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.6) KNN implementation","metadata":{}},{"cell_type":"code","source":"\n#kNN\ndef kNN(data, labels, new, dist_func, k):\n    distances = []\n    for idx in range(len(data)):\n        distances.append([dist_func(data[idx],new),labels[idx]])\n    distances = sorted(distances)\n    assigned_label = {0:0 , 1:0}\n    for idx in range(k):\n        assigned_label[distances[k][1]] += 1\n    return max(assigned_label,key = assigned_label.get)  ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:37:48.657513Z","iopub.execute_input":"2021-12-16T21:37:48.657781Z","iopub.status.idle":"2021-12-16T21:37:48.663780Z","shell.execute_reply.started":"2021-12-16T21:37:48.657756Z","shell.execute_reply":"2021-12-16T21:37:48.663012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distance func:\ndef L1_dist(arr1,arr2):\n    res = 0\n    for i in range(len(arr1)):\n        res += abs(arr1[i] - arr2[i])\n    return res\ndef L2_dist(arr1,arr2):\n    return np.linalg.norm(arr1-arr2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\ntotal = 0\nx = x_train\ny = y_train\nfor idx in range(100):\n    guess = kNN(x,y,x_test[idx],L2_dist,100)\n    if guess == y_test[idx]:\n        correct += 1\n    total += 1\nprint(\"Accuracy: \", round(correct/total, 3))","metadata":{},"execution_count":null,"outputs":[]}]}